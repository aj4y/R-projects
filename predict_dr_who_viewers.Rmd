---
title: "Train and evaluate models with tidymodels"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5)
```


1. Ratings vary from 75 to 95, with average around 85
2. Number of viewers have declined over the years overall for regular and sepcial type of episodes
3. Writer and director of the episode seems to have an impact on rating


```{r}
library(tidymodels)
library(tidyverse)
library(tidytext)


data <- tidytuesdayR::tt_load('2021-11-23')

episodes <- data$episodes

episodes%>%
  ggplot(aes(rating))+geom_histogram()

episodes%>%
  ggplot(aes(uk_viewers))+geom_histogram()

episodes %>% 
  ggplot(aes(first_aired,uk_viewers,color = type))+geom_line()+
  labs(x = "First aired date",
       y = "No of viewers",
       title = 'Number of viewers over the yerar',
       color = "Type")

writers <- data$writers
directors <- data$directors
imdb <- data$imdb


episodes %>% count(story_number,sort=T)

episodes_joined <- 
  episodes %>% 
  left_join(.,writers)%>%
  left_join(.,directors)%>%
  left_join(.,imdb %>% 
              select(season,ep_num,rating_n,desc)%>%
              rename(season_number=season,episode_number=ep_num))%>%
  filter(!is.na(uk_viewers),!is.na(duration))

episodes_joined %>% 
  glimpse()
  

episodes_joined %>% 
  mutate(writer = fct_lump(writer,10))%>%
  ggplot(aes(rating))+geom_histogram()+facet_wrap(~writer)

episodes_joined %>% 
  mutate(director = fct_lump(director,10))%>%
  ggplot(aes(rating))+geom_histogram()+facet_wrap(~director)


episodes_joined %>% 
  group_by(writer)%>%
  summarise(n=n(),mean_rating=mean(rating),max_rating = max(rating),min_rating = min(rating))%>%
  mutate(writer = fct_reorder(writer,mean_rating))%>%
  ggplot(aes(mean_rating,writer))+geom_point()+geom_errorbarh(aes(xmax = max_rating,xmin = min_rating))


episodes_joined %>% 
  group_by(director)%>%
  summarise(n=n(),mean_rating=mean(rating),max_rating = max(rating),min_rating = min(rating))%>%
  mutate(director = fct_reorder(director,mean_rating))%>%
  ggplot(aes(mean_rating,director))+geom_point()+geom_errorbarh(aes(xmax = max_rating,xmin = min_rating))


episodes_joined %>% 
  filter(is.na(duration))



```


## Build models

Let's consider how to [spend our data budget](https://www.tmwr.org/splitting.html):

- create training and testing sets
- create resampling folds from the *training* set

```{r}
set.seed(123)
episodes_split <- initial_split(episodes_joined, strata = uk_viewers)
episode_train <- training(episodes_split)
episode_test <- testing(episodes_split)

set.seed(234)
episode_folds <- vfold_cv(episode_train)
episode_folds
```

Let's create a [**model specification**](https://www.tmwr.org/models.html) for each model we want to try:

```{r}

episode_rec <- 
  recipe(uk_viewers ~ type + duration , data = episode_train)%>%
  #step_other(writer,threshold = 0.1)%>%
  #step_other(director,threshold = 0.1)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_numeric_predictors())


episode_rec %>% prep()%>%bake(new_data=NULL)

ranger_spec <-
  rand_forest(trees = 1e3) %>%
  set_engine("ranger") %>%
  set_mode("regression")

```

To set up your modeling code, consider using the [parsnip addin](https://parsnip.tidymodels.org/reference/parsnip_addin.html) or the [usemodels](https://usemodels.tidymodels.org/) package.

Now let's build a [**model workflow**](https://www.tmwr.org/workflows.html) combining each model specification with a data preprocessor:

```{r}
ranger_wf <- workflow(episode_rec, ranger_spec)
ranger_wf

```

If your feature engineering needs are more complex than provided by a formula like `sex ~ .`, use a [recipe](https://www.tidymodels.org/start/recipes/). [Read more about feature engineering with recipes](https://www.tmwr.org/recipes.html) to learn how they work.


## Evaluate models

These models have no tuning parameters so we can evaluate them as they are. [Learn about tuning hyperparameters here.](https://www.tidymodels.org/start/tuning/)

```{r}
contrl_preds <- control_resamples(save_pred = TRUE)


ranger_rs <- fit_resamples(
  ranger_wf,
  resamples = episode_folds,
  control = contrl_preds
)

```

Model metrics look worse

```{r}

collect_metrics(ranger_rs)
```


These models perform very similarly, so perhaps we would choose the simpler, linear model. The function `last_fit()` *fits* one final time on the training data and *evaluates* on the testing data. This is the first time we have used the testing data.

```{r}
final_fitted <- last_fit(ranger_wf, episodes_split)
collect_metrics(final_fitted)  ## metrics evaluated on the *testing* data
```


1. Predictions are horrible 

```{r}
final_wf <- extract_workflow(final_fitted)
episode_test %>%
  select(uk_viewers)%>%
  cbind(predict(final_wf, episode_test,interval ='confidence'))%>%
  ggplot(aes(uk_viewers,.pred))+geom_point()
```

You can save this fitted `final_wf` object to use later with new data, for example with `readr::write_rds()`.
